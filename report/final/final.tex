
% GRADING RUBRIC:
% Task definition: is the task precisely defined and does the formulation make sense?
% Approach: was a baseline, an oracle, and an advanced method described clearly, well justified, and tested?
% Data and experiments: have you explained the data clearly, performed systematic experiments, and reported concrete results?
% Analysis: did you interpret the results and try to explain why things worked (or didn't work) the way they did? Do you show concrete examples?


\documentclass[journal]{IEEEtran}
\usepackage{graphicx, amsmath, amssymb, epstopdf}
\usepackage{graphicx}
\usepackage{url}

% correct bad hyphenation here
\hyphenation{op-tical net-works semi-conduc-tor}


\begin{document}
\title{Predicting movie ratings}

\author{Carolyn~Au~\IEEEmembership{auc@stanford.edu},
        Justin~Cunningham~\IEEEmembership{jcnnghm@stanford.edu},
        and~Weixiong~Zheng~\IEEEmembership{zhengwx@stanford.edu}}

\markboth{CS 221 (Autumn 2014) Project Final Report}%
{}

\maketitle


\begin{abstract}
The project aims to explore machine learning methods to predict a movie's critical success prior
to it's release, i.e. based solely on metadata available about the movie. The code for this project
can be found at \texttt{\url{https://github.com/jcnnghm/cs221-project}}
\end{abstract}


\section{Overview}
The movie industry generates multi-billion dollars in revenue and most movies
cost millions of dollars to create.  However, not all movies are successes.
With such high risk and large amounts of money involved, it would be useful to
be able to predict the success of a movie before it is released. In this
project, we try to predict the critical success of a movie, as shown by user
ratings on IMDb \cite{imdb}, based solely on metadata about a movie available prior to it's
release.
\\
\begin{itemize}
    \item Input: IMDb metadata for a movie related to it's creation and release
    \item Output: Predicted user rating of a movie, on a scale of 0-10
\end{itemize}


\section{Related Work}
Henning-Thurau, Houston and Walsh \cite{marketing} performed an empirical study
to distinguish direct and indirect relationships among different determinants
of movie success and found that star and director power does not guarantee
success. However, cultural familiarity (e.g. sequels to a successful movie),
release dates (e.g. during the summer), budget and awards were a huge predictor
of success. Relatedly, Deniz and Hasbrouck \cite{greenlight} performed
statistical analysis on the top 150 grossing movies of 2010 and found that
genre, MPAA rating, budget, star power, adaptation from another medium, sequels
and remakes are significant predictors
of box office revenue.
\\
\par There are also other student projects that attempted to predict user
ratings. One was done for the Machine Learning class (CS229) \cite{hitorflop},
where they used Naive Bayes and Support Vector Machines to predict IMDb user
ratings and profitability. Their system performed moderately well on their test
data. Another similar project \cite{cooper} used regression (Support Vector
Regression, Boosted Decision Trees, Gradient Boosting Regression and Random
Forest Regression) over a different dataset (The Sagel Index of the top and
worst 1000 films), predicting audience ratings on Rotten Tomatoes. Their system
resulted in error rates of roughly 10\%.

\section{Dataset}
Our dataset is the list of all movies from IMDb that fulfill the following properties
\\
\begin{itemize}
	\item Released in the US
	\item Generated gross earnings in the US
	\item Has at least 1,000 user votes to rate the movie
\end{itemize}
\bigskip

\par After pruning the database of 3 million entries, we are left with 9,888
movies which is a reasonable number for our algorithms to run on. Limiting the
data to movies with a reasonable number of user votes also ensures that the
rating data is not too noisy. We save 20\% of the data for testing, and use the
rest for development.

\scalebox{0.55}{\includegraphics{ratings.eps}}

The user ratings in our dataset have a mean of 6.5 and a median of 6.6 with a standard deviation of 1.05.

\section{Evaluation Metric}
We used the standard deviation of the predicted user rating from the actual values on IMDb, calculated as

$$ \sqrt{\frac{\sum_{i=1}^n(\text{Predicted}_i-\text{Actual}_i)^2}{n}} $$

\subsection{Baseline}
For our baseline, we included the following features:
\\
\begin{itemize}
	\item Complete cast members, which includes actors, directors, producers, etc.
	\item Movie genre
	\item Keywords describing the movie
\end{itemize}
\bigskip

\par This results in a standard error of 0.06 on the training data, and
1.78 on the test data.

\subsection{Oracle}
For our oracle, we added additional features which are movie data available
\emph{after} a movie is released. These are:
\\
\begin{itemize}
	\item Gross earnings
	\item IMDb user ratings
	\item Number of votes for the ratings
\end{itemize}
\bigskip
\par This results in a standard error of of 0.03 on the training data, and 0.34 on the test data.
\\
\par We had initially added critic and audience ratings from Rotten Tomatoes, but it produced unexpected results, namely critic scores were negative weights and negative audience ratings had the opposite effect (movies rated \emph{Spilled} had larger weights than \emph{Upright})

\section{System design}
Our system is composed of a feature creator system, a snapshot of the IMDb dataset,
and a variety of learning algorithms that operate on the information exported by
the feature creator.

\begin{center}
    \includegraphics[width=8cm]{charts/system.eps}
\end{center}

\par The feature creator provides a framework that allows sub-component
feature extractors to interact with the IMDb dataset, stored in MySQL.  The feature
creator calls each extractor ($E_i$ in the diagram below) in turn for each of the movies we've selected for our
dev and test sets, and merges their output features into a sparse feature vector for
each movie.  An additional combinator step ($C_i$ in the diagram) is applied to generate feature
combinations, e.g. \{movie director $\times$ movie actors\}.
\\
\par The creator is highly configurable so that extractors and combinators can easily be selectively enabled or disabled,
and includes a caching layer to reduce the load on MySQL and speed up processing.
The feature vectors are stored in JSON so that they can be easily
post-processed for use with different algorithms.
\\
\par Each algorithm is responsible for transforming the data to the format that it requires.
The neural network, for example, converts the sparse vectors from the feature creator
into a dense representation that can be consumed by FANN \cite{fann}, a neural network library.  Each
component is modular, so that it is easy to tune individual algorithms.


\section{Features}
The features we used can be catogrized into 3 different types.

\par The first type of features are the original data we get from IMDB without any modifications, such as cast, genere, etc. These features form the basis of the feature set for our Machine Learning algorithms. However, some of them don't generalize well. For example, as time goes by, new names would keep showing up in casts. Most of the weights in our models for old casts won't help in predicting ratings for new movies.

\par The second type of features are combined features, such as \{movie director $\times$ movie actors\}. With these features, we want to capture the significance of relationships between some individual features. For example, an actor might work really well with a specific director. These combined features also don't generalize well in some cases.

\par The third type of features are computed features such as cast experience and standarized budget. These features involved thoughtful analysis and computation. These computed features generalized pretty well across training and test samples. These features helped a lot in pushing the boundary of prediction accuracy.

\par Below is a detailed description of each class of features we include along with an example of the features we generate.

\subsection{Individual features}
\subsubsection{Budget}
The Budget feature extractor extracts a weighted feature, within the domain of
\$1 to \$300M.  Since the data is crowdsourced, there are multiple specified
budgets for the movie.  We only consider movies with budgets in US dollars, and
we take the largest proposed budget.  We also consider using budgets in two
ways -- using raw dollar values or bucketing the value into \$5M chunks (as
indicator variables).
\begin{align*}
        \{ \ \ & \\
        &``budget": 10000000,\\
        &``budget\_bucket\_2: 1\\
        \} \ \ &
\end{align*}

\subsubsection{Cast}
The cast feature extractor generates indicator features for each
cast and crew member associated with a film, for each job they
performed.  An individual could potentially appear more than once,
if they performed different jobs, for example, if they were both
an actor and director.  The cast roles which we include are:
\begin{itemize}
	\item Actor
	\item Actress
	\item Producer
	\item Writer
	\item Cinematographer
	\item Composer
	\item Costume designer
	\item Director
	\item Editor
	\item Production designer
	\item Miscellaneous crew
\end{itemize}
\begin{align*}
        \{ \ \ & \\
        &``Lasseter,John\_producer": 1,\\
        &``Cummings,Jim\_actor": 1,\\
        &``Mitchell,Nicole\_writer": 1,\\
        \} \ \ &
\end{align*}

\subsubsection{Genre}
Genre features are indicator features that are activated for each
genre associated with a movie in the IMDb dataset.
\begin{align*}
        \{\ \ &\\
        &``genre\_Family": 1,\\
        &``genre\_Comedy": 1,\\
        \}\ \ &
\end{align*}

\subsubsection{Keywords}
Keyword features are indicator features that are activated for
each searchable tag that is associated with a movie in the
IMDb dataset.
\begin{align*}
        \{\ \ &\\
        &``character-name-in-title": 1,\\
        &``sequel": 1,\\
        &``friendship": 1,\\
        \}\ \ &
\end{align*}

\subsubsection{Release Date}
Release dates are also indicator features that are activated for each release
in the US across festivals and premieres. For each release, we generate 3
features, one for the month, year, and both month and year.
\begin{align*}
        \{\ \ &\\
        &``release\ (traverse\ film\ festival)\ (August)": 1, \\
        &``release\ (traverse\ film\ festival)\ (2014)": 1, \\
        &``release\ (traverse\ film\ festival)\ (August\ 2014)": 1, \\
        &``release\ (premiere)\ (February)": 1, \\
        &``release\ (premiere)\ (2014)": 1, \\
        &``release\ (premiere)\ (February\ 2014)": 1, \\
        \}\ \ &
\end{align*}

\subsubsection{Budget $\times$ Cast}
We use the bucketized budget data and join that with the extracted cast
features. The idea is that an actor's star power will be amplified by a big
budget production, or reduced in an indie movie.
\begin{align*}
        \{\ \ &\\
        &``budget\_bucket\_2\_Cummings,Jim\_actor": 1,\\
        &``budget\_bucket\_2\_Klein,Sebastian\_actor": 1\\
        \}\ \ &
\end{align*}

\subsubsection{Actor $\times$ Director}
We create new features for each combination of actor and director in each cast
list (where actor includes both actors and actresses, treated distinctly in our
dataset). The intuition is that some pairs of actors and directors work well
together and create better movies together.
\begin{align*}
        \{\ \ &\\
        &``Nolan,Christopher\_Cummings,Jim\_actor": 1,\\
        &``Nolan,Christopher\_Klein,Sebastian\_actor": 1\\
        \}\ \ &
\end{align*}

\subsubsection{Standardized budget}
In order to account for inflation, we added a feature which calculates the
standard score of the movie budget based on other movies released that same
year.  The standard score of the movie budget for a movie $M$ released in year
$Y$ is given by

\begin{eqnarray*}
    z = \frac{x - \mu}{\sigma}
\end{eqnarray*}
where
\begin{itemize}
    \item $z$ is the standard score of the budget for movie $M$
    \item $\mu$ is the mean of movie budgets released in year $Y$
    \item $\sigma$ is the standard deviation of movie budgets released in year $Y$
\end{itemize}
\begin{align*}
        \{\ \ &\\
        &``standardized\_budget": 0.57408881370714016,\\
        \}\ \ &
\end{align*}

\subsubsection{Cast and Crew Experience}
\paragraph{Experience}

To better generalize for cast and crew we had not trained on, we added features that would describe how much experience the cast and crew had.  For the buckets of all Actors, Actresses, Producers, Directors, the group of of all Actors and Actresses, and the entire cast and crew, we calculated the mean number of movies they had previously worked on or appeared in.  We also calculated the 1st, 10th, 25th, 50th, 75th, 90th, and 99th percentiles for each group.
\begin{center}
    \scalebox{0.5}{\includegraphics{charts/appearances.eps}}
\end{center}

The total number of movies a person had worked on was capped at 20, and each role-value pair was treated as an indicator feature, with values rounded to the nearest integer.
\begin{align*}
        \{\ \ &\\
        &``experience\_Actors\_mean\_14": 1,\\
        &``experience\_ActorsAndActresses\_mean\_16": 1,\\
        &``experience\_Actors\_10\_pctl\_3": 1,\\
        &``experience\_Actors\_90\_pctl\_16": 1,\\
        \}\ \ &
\end{align*}

\paragraph{Quality}

Similar to cast and crew experience, we added features that would describe the quality of the cast and crew based on the rating of movies that they had worked on or appeared in previously.  The mean and percentiles approach as described above was also used for cast and crew quality.
\begin{center}
    \scalebox{0.45}{\includegraphics{charts/crew_quality.eps}}
\end{center}

After the mean or percentile value was calculated, it was rounded to the nearest integer, and along with the role, added as an indicator feature.
\begin{align*}
        \{\ \ &\\
        &``rating\_Actors\_mean\_6": 1,\\
        &``rating\_ActorsAndActresses\_mean\_7": 1,\\
        &``rating\_Actors\_10\_pctl\_4": 1,\\
        &``rating\_Actors\_90\_pctl\_8": 1,\\
        \}\ \ &
\end{align*}

\subsubsection{K-means}
The k-means cluster for each movie is added to each movie's feature vector.
Adding just 10 clusters provided a significant performance improvement after
1,000 iterations of SGD.  The k-means algorithm was initialized with 128 different
centroid seeds using the k-means++ method, the best result is retained.  Total
runtime was less than 4 minutes.  All of our algorithms included k-means clusters as features.

\begin{center}
\begin{tabular}{|r| r|} % columns
\hline
Num. clusters & Test Error  \\ [0.5ex] % inserts table
\hline
0 & 1.75 \\
10 & 0.97 \\
100 & 1.02 \\
1000 & 1.32 \\
\hline %inserts single line
\end{tabular}
\end{center}

Clusters added were treated as indicator features.
\begin{align*}
        \{\ \ &\\
        &``cluster\_4": 1,\\
        \}\ \ &
\end{align*}

\subsection{Analysis of feature sets}
In total, there are about 192,000 unique features without combined features for
our data set.  With combined features, this number balloons to over 800,000.
\begin{center}
    \scalebox{0.55}{\includegraphics{charts/features.eps}}
\end{center}

When represented sparsely, the feature size without combinations is about 53MB, and 82MB with combinations.  The same data represented
densely consumes approximately 3.5GB and 14GB respectively, indicating that many of the features are not associated with many movies.  We chose to prune features that do not represent at least 3 movies, when
those are filtered out, we're left with approximately 120,000 features, with the average feature
representing 20.2 movies. 97.9\% of our features represent fewer than 100 movies.  The data
is skewed heavily to the left, which has the tendency to cause overfitting, which we address in
our algorithms. \\
\\
    \scalebox{0.45}{\includegraphics{charts/movies_per_feature.eps}}
\\
\par An initial investigation into the importance of each feature was performed by
running our baseline algorithm with each feature set individually.\\

\begin{center}
\begin{tabular}{|l|r r|} % columns
\hline
Feature & Dev error & Test error \\ [0.5ex] % inserts table
\hline % inserts single horizontal line
Budget (raw) & 5.89 & 5.87 \\ % inserting body of the table
Budget (bucketized) & 4.62 & 4.68 \\
Cast & 0.65 & 2.56 \\
Genre & 2.26 & 2.29 \\
Keywords & 1.65 & 3.35 \\
Release date & 1.69 & 2.26 \\
\hline
Budget $\times$ Cast & 4.54 & 5.65 \\
Actor $\times$ Director & 2.67 & 5.96 \\
\hline
Standardized budget & 5.02 & 5.03 \\
Cast experience & 0.80 & 0.90 \\
\hline
\end{tabular}
\end{center}
\smallskip

\par Here we note that some feature sets are overfitted to our dev dataset such as
cast, keywords, release date, budget $\times$ cast and actor $\times$ director.
These are also our largest feature sets.\\
\\
\begin{tabular}{|l| r r r|} % columns
\hline
Feature             & \shortstack[c]{Num.\\vars} & \shortstack[c]{Num.\\movies} & \shortstack[c]{Feature / \\ movie}  \\ [0.5ex] % inserts table
\hline
Budget (raw)        & 1 & 5,278 (53\%) & 1 \\ % inserting body of the table
Budget (bucketized) & 52 & 5,278 (53\%) & 1 \\
Cast                & 169,973 & 9,883 (99\%) & 72 \\
Genre               & 25 & 9,885 (99\%) & 2 \\
Keywords            & 11,701 & 9,774 (98\%) & 75 \\
Release date        & 10,500 & 9,884 (99\%) & 7 \\
\hline
Budget $\times$ Cast & 394,661 & 5,278 (53\%) & 92 \\
Actor $\times$ Director & 282,672 & 8,630 (87\%) & 35 \\
\hline
Standardized budget & 1 & 5,278 (53\%) & 1 \\
Cast experience & 1,230 & 9,886 (99\%) & 94 \\
\hline %inserts single line
\end{tabular}

\subsection{Feature pruning}
With combined features, our feature space size is around 800,000. It takes hours to train a Linear Regression Model using scikit-learn \cite{scikit} on such a large sample space. We did some experiments on feature removal to improve algorithm performance.
\\
\par In our experiments, we removed all the features that appear less or equal to $n$ times in the training data before training a model. We measured the number of features left after removal and the standard error on test set.\\
\begin{center}
\begin{tabular}{|l| r r|} % columns
\hline
$n$ & Num. features left & Standard Error  \\ [0.5ex] % inserts table
\hline
0 & 723,000 & N/A \\
1 & 188,000 & 0.79 \\
2 & 93,000  & 0.83 \\
9 & 28,000  & 1.12 \\
\hline %inserts single line
\end{tabular}
\end{center}
\smallskip

\textit{delete or update this paragraph?}
\par Removing the features that
appear only once in the training set gives huge improvements on performance
while maintaining low standard errors on fitting the test set. This makes sense
because if a
feature appears only once in the training set, it's highly likely that the
feature won't appear in the test set, which means that the weight for this feature
won't affect the result of fitting the test set. Also, according to the experiment,
increasing the threshold $n$ doesn't give too much performance gain but worsens
the test error.

\subsection{Principal Component Analysis}
We ran Principal Component Analysis, PCA, on our features to further reduce
our feature size for the neural network. With PCA, we
are able to reduce our feature set to 5,000.  The first feature
represents about 1.5\% of the variance, with the first 5,000 features
capturing 91\% of the variance in the data.
\\
\scalebox{0.575}{\includegraphics{charts/pca.eps}}
\\
\scalebox{0.55}{\includegraphics{charts/pca_variance.eps}}

\section{Algorithms}

\subsection{Linear Regression}
We used the implementation of Linear Regression in scikit-learn with default
parameters. The algorithm took 6 seconds to run when the data was represented
sparsely, and gave a standard error of 0.81.
\\
\par Linear regression was somewhat susceptible to overfitting, and is not tunable, so we used
linear regression primarily to quickly check work in progress,

\subsection{Stochastic Gradient Descent}

We obtained the best results running SGD with a regularization constant of 0.01 over 100,000 iterations.  Total runtime was approximately 21 minutes, and peak memory consumption was approximately 20GB.
\\
\par We were able to obtain a standard error of 0.7197, over 1 point better than our baseline,
>>>>>>> upstream/master
and only 0.38 from our oracle.
\\
\subsubsection{Iterations and Regularization}
We experimented with many different iteration counts and regularization constants, ultimately finding that 100,000 iterations and a regularization constant of 0.01 provided the best results.  Our data was highly susceptible to overfitting.  Using a low number of iterations and either no regularization or a regularization constant $ \lambda <= 0.001 $  provided the best results.

\scalebox{0.55}{\includegraphics{charts/sgd_iterations.eps}}
\scalebox{0.55}{\includegraphics{charts/sgd_regularization.eps}}

While we were able to return acceptable results with early stopping, achieving standard errors as low as 0.76, our results were not as consistent as they were with a large number of iterations and a tuned regularization constant.  Our SGD implementation takes fairly large steps in its earliest iterations, and makes progressively smaller updates as it runs.  Since our algorithm was converging well, we chose not to adjust the step size, meaning we didn't have as much control of the test error using early stopping as we did by tuning the regularization constant.

\subsection{Logistic Regression}

We ran logistic regression using scikit-learn with default parameters.  The algorithm exhibited overfitting, with a training error of 0.0 and a relatively high test error of 0.9157.  We chose to focus on SGD instead of logistic regression because our data is prone to overfitting, and we were able to obtain better preliminary results and had greater control over the hyperparameters in the SGD implementation.  The total runtime was approximately 85 seconds.

\subsection{Neural Network}
The neural network is implemented using the Fast Artificial Neural Network (FANN)
API.  With three hidden layers, with 40 neurons in the first, 20 in the second, and 10 in the last,
we have achieved a test error rate of 0.000926, when the rating is represented as a
number between 0 and 1.0, corresponding to an average error of approximately 0.82 out of 10.

\\
\par Our initial implementation of the neural network was quite slow, taking approximately
12 hours to complete 100 iterations with a simple topology consisting of one hidden
layer with 10 neurons.  This is due, in part, to the lack of support for sparse matrices in the FANN library, and the relatively large number of features we were training on.  This slowness made working with the neural network impractical, so we used PCA to reduce the number of features to 5,000 from 93,000, resulting in runtimes near 1 hour.  This feature reduction resulted in a loss of approximately 9\% of the variance in the data, artificially limiting the performance of the network.  \\

\scalebox{0.55}{\includegraphics{charts/neural_network.eps}}
\\
\par Additionally, the neural network was susceptible to severe overfitting.  The FANN library doesn't have much built in support to prevent overfitting; the suggested approach is early stopping.  We did implement early stopping using a simple heuristic that proved to be effective, partitioning the data and stopping when the training error was falling but the test error started rising, after a period of falling.  We were able to get acceptable results using that approach, but the lack of the ability to tune regularization hyperparameters, the limited control provided by early stopping, and the loss of variance reduced the neural networks performance below the performance of simpler, more flexible models.
\\
\par More data and more generalized features, like the cast experience features, would likely help.  However, increasing the amount of data isn't necessarily feasible without adversely impacting the performance of the network, likely necessitating greater compute resources as well.  Other neural network implementation that provide better support for regularization and support sparse matrices would also be helpful.

\section{Results}
\smallskip
\begin{center}
\begin{tabular}{|l | l|} % columns
\hline
Model               & Standard Error  \\ [0.5ex] % inserts table
\hline
Baseline            & 1.78 \\
\hline
Linear regression  & 0.81 \\
Logistic regressor  & 0.92 \\
SGD regressor       & 0.82 \\
Neural network      & 0.82 \\
SGD with $ \lambda = 0.01 $ \& 100k iterations & 0.72 \\
% SVM (rbf kernel)    & 1.09 \\
% SVM (linear kernel) & 1.02 \\
\hline
Oracle              & 0.34 \\
\hline %inserts single line
\end{tabular}
\end{center}
\smallskip


\section{Analysis}

\subsection{Algorithms}
Linear algorithms are quick to train (60 seconds compared with over 10 hours
for a neural network without PCA) and are not as susceptible to overfitting.
They are also not as resource intensive to compute.
\\
\par Additionally, some benefits of the specific implementations of linear algorithms we used are:
\begin{itemize}
    \item Easier to avoid overfitting with regularization hyperparameters
    \item Sparse matrix support, data sizes are 1GB
\end{itemize}
\bigskip
\par While we found that with the FANN neural networks:
\begin{itemize}
    \item Limited tunable hyperparameters
    \item Difficult to avoid overfitting -- early stopping heuristic does a decent job however
    \item Dense matrices required
\end{itemize}
\bigskip
\par Neural Networks are able to learn more complex models, but this additional
power can lead to overfitting and massive resource requirements.
\\
\par Reducing the feature set and stopping early helps with this, but in this
case, these compromises result in performance comparable to the linear
algorithms.

\subsection{Features}

We used three different types of features to train the models, base features, combined features, and computed features. Adding combined features expanded the feature space size by more than 4 times, causing a few models to overfit. To handle overfitting and improve performance, we added a preprocess step to remove less common features. By removing the features that appear less than 3 times among all movies, we effectively reduced the feature space size by a factor of 5. However, one downside of the base features and combined features is they don't generalize too well. A lot of these features only appear a few times through out the whole dataset.

\par The computed features generalize very well. For example, cast experience is a feature that can be computed for every movie. Adding these computed features really helped to push the boundary of test errors. By adding the computed features, we successfully reduced test errors by more than 4\%.

\section{Future work}
Our ability to go further with Neural Networks was hampered by the amount of
compute resources required to process our full feature set. It would be
interesting to see what the results would be with all our features. The models
could also be improved further with additional data, such as an actor's star
power (through sentiment analysis of social media) or the marketing budgets of
the movie (unavailable through IMDb). We also saw a large improvement in our
prediction accuracy by adding generalized features such as the cast and crew
experience feature. Other generalized features to experiment with would be the
number of movies released at the same time.

\section{Acknowledgements}
We would like to thank Professor Liang for the opportunity to work on this
project for CS 221, and to the course staff, particularly our project mentor
Ilan Goodman for his guidance and support.

\section{Addendum}
As an aside, we thought it would be fun to find out what metadata would
generate the most successful movie by looking at the highest and lowest
weighted features generated by SGD.
A guaranteed hit as judged by our system would be a rather strange movie: a stop-motion animated documentary based on a play about a cartoon mutant dog set in Italy made with a budget of \$60M starring Dennis Quaid. Conversely, a movie that we recommend against making would be a horror movie featuring giant animals and psychotronics.
% \begin{center}
% \begin{tabular}{|l|l|l|} % columns
% \hline
% Feature & Data & Weight \\ [0.5ex]
% \hline
%     Budget & \$60,000,000 & 0.03 \\
%     Budget & \$55,000,000 & 0.03 \\
%     Budget & \$100,000,000 & 0.03 \\
%     Budget & \$105,000,000 & 0.02 \\
%     Budget & \$95,000,000 & 0.02 \\
% \hline
%     Genre & Documentary & 0.60 \\
%     Genre &Animation & 0.37 \\
%     Genre &Drama & 0.25 \\
%     Genre &Music & 0.15 \\
%     Genre &Sport & 0.15 \\
% \hline
%     Release & New York in March & 0.23 \\
%     Release & \shortstack[l]{Independent Film Festival of Boston\\in April}& 0.17 \\
%     Release & Limited release in September & 0.15 \\
%     Release & Austin Film Festival in October& 0.15 \\
%     Release & New York Film Festival in October& 0.14 \\
% \hline
%     Keyword & stop-motion-animation & 0.29 \\
%     Keyword & based-on-play & 0.25 \\
%     Keyword & cartoon-dog & 0.22 \\
%     Keyword & mutant & 0.20 \\
%     Keyword & splatter & 0.20 \\
%     Keyword & italy & 0.20 \\
%     Keyword & splatter & 0.20 \\
%     Keyword & ampersand-in-title & 0.19 \\
%     Keyword & first-part & 0.18 \\
%     Keyword & cliff & 0.18 \\
% \hline
%     Actor & Jim Fitzpatrick & 0.22 \\
%     Actor & Ashish Vidyarthi & 0.16 \\
%     Actor & Dennis Quaid & 0.13 \\
%     Actor & George W. Bush & 0.12 \\
%     Actor & Stellan Skarsgård & 0.11 \\
%     Actor & Donald Trump & 0.11 \\
%     Actor & William Shatner & 0.11 \\
%     Actor & Jon Gries & 0.11 \\
%     Actor & Alan Oppenheimer & 0.11 \\
% \hline
%     Actress & Tabu & 0.12 \\
%     Actress & Bipasha Basu & 0.11 \\
%     Actress & Divya Dutta & 0.11 \\
%     Actress & Susan Tyrrell & 0.10 \\
%     Actress & Kiran Juneja & 0.10 \\
%     Actress & Milena Vukotic & 0.09 \\
%     Actress & Dolly Ahluwalia & 0.09 \\
%     Actress & Mona Marshall & 0.09 \\
%     Actress & Karen Strassman & 0.09 \\
%     Actress & Elizabeth Daily & 0.09 \\
% \hline
%     Producer & Sunil Lulla & 0.12 \\
%     Producer & Siddharth Roy Kapur & 0.12 \\
%     Producer & Aashish Singh & 0.11 \\
%     Producer & Peter Newman & 0.11 \\
%     Producer & Padam Bhushan & 0.10 \\
% \hline
%     Director & Imtiaz Ali & 0.09 \\
%     Director & Vishal Bhardwaj & 0.09 \\
%     Director & Don Coscarelli & 0.08 \\
%     Director & Jonathan Demme & 0.08 \\
%     Director & Tony Gatlif & 0.07 \\
% \hline %inserts single line
% \end{tabular}
% \end{center}

% \textit{Explain these}
% \begin{itemize}
%     \item why bollywood
%     \item why b-grade actors
%     \item why nyc in march
% \end{itemize}
%
% \subsection{Learned weights}
% Top 10 weights:
% \begin{center}
% \begin{tabular}{|l|l|l|} % columns
% \hline
% Feature & Data & Weight \\ [0.5ex]
% \hline
% Genre & Documentary & 0.60 \\
% Crew experience ($1^{st}$ pctl) & 1 & 0.52 \\
% Cast experience ($99^{th}$ pctl) & 1 & 0.43 \\
% Crew experience ($10^{th}$ pctl) & 3 & 0.41 \\
% Genre & Animation & 0.37 \\
% Crew ratings ($75^{th}$ pctl) & 8 & 0.32 \\
% Crew ratings (mean) & 7 & 0.32 \\
% Crew ratings (mean) & 8 & 0.31 \\
% Crew experience ($10^{th}$ pctl) & 1 & 0.31 \\
% Crew ratings ($50^{th}$ pctl) & 8 & 0.30 \\
% \hline %inserts single line
% \end{tabular}
% \end{center}
%
% Bottom 10 weights:
% \begin{center}
% \begin{tabular}{|l|l|l|} % columns
% \hline
% Feature & Data & Weight \\ [0.5ex]
% \hline
% Genre & Horror & -0.49 \\
% Keyword & critically-bashed & -0.46 \\
% Keyword & box-office-flop & -0.44 \\
% Keyword & b-movie & -0.30 \\
% Keyword & giant-animal & -0.26 \\
% Cast experience & 2 & -0.22 \\
% ($99^{th}$ pctl.) &   &       \\
% Release & New York City (2013) & -0.22 \\
% Release & Los Angeles (July 2013) & -0.22 \\
% Keyword & sequel & -0.21 \\
% Crew member & Prakash, Chinni & -0.21 \\
% \hline %inserts single line
% \end{tabular}
% \end{center}

% references section

% can use a bibliography generated by BibTeX as a .bbl file
% BibTeX documentation can be easily obtained at:
% http://www.ctan.org/tex-archive/biblio/bibtex/contrib/doc/
% The IEEEtran BibTeX style support page is at:
% http://www.michaelshell.org/tex/ieeetran/bibtex/
%\bibliographystyle{IEEEtran}
% argument is your BibTeX string definitions and bibliography database(s)
%\bibliography{IEEEabrv,../bib/paper}
%
% <OR> manually copy in the resultant .bbl file
% set second argument of \begin to the number of references
% (used to reserve space for the reference number labels box)
\begin{thebibliography}{1}
\bibitem{imdb}
Internet Movie Database: \texttt{\url{http://imdb.com}}

\bibitem{marketing}
H-T Thorsten, H. Mark, W. Gianfranco, \emph{Determinants of Motion Picture Box
Office and Profitability: An Interrelationship Approach} \hskip 1em plus 0.5em
minus 0.4em\relax Review of Managerial Science, 2006.

\bibitem{greenlight}
B.~Deniz and R.~B.~Hasbrouck, \emph{WHEN TO GREENLIGHT:
Examining the Pre-release Factors that Determine Future Box Office Success of a Movie in the United States}
\hskip 1em plus 0.5em minus 0.4em\relax International Journal of Economics and Management Sciences, 2012.

\bibitem{hitorflop}
D.~Cocuzzo and S.~Wu, \emph{Hit or Flop: Box Office Prediction for Feature
Films}\hskip 1em plus
  0.5em minus 0.4em\relax Stanford CS 229 project, Dec 2013.

\bibitem{cooper}
S.~Mevawala and S.~Phadke, \emph{BoxOffice: Machine Learning Methods for Predicting Audience Film Ratings}\hskip 1em plus
  0.5em minus 0.4em\relax The Cooper Union for the Advancement of Science and Art.

\bibitem{fann}
Fast Artificial Neural Network Library: \texttt{\url{http://leenissen.dk/fann/wp/}}

\bibitem{scikit}
Scikit-learn: \texttt{\url{http://scikit-learn.org/stable/}}
\end{thebibliography}

% that's all folks
\end{document}


