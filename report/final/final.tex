
% GRADING RUBRIC:
% Task definition: is the task precisely defined and does the formulation make sense?
% Approach: was a baseline, an oracle, and an advanced method described clearly, well justified, and tested?
% Data and experiments: have you explained the data clearly, performed systematic experiments, and reported concrete results?
% Analysis: did you interpret the results and try to explain why things worked (or didn't work) the way they did? Do you show concrete examples?


\documentclass[journal]{IEEEtran}
\usepackage{graphicx, amsmath, amssymb, epstopdf}
\usepackage{graphicx}
\usepackage{url}

% correct bad hyphenation here
\hyphenation{op-tical net-works semi-conduc-tor}


\begin{document}
\title{Predicting movie ratings}

\author{Carolyn~Au~\IEEEmembership{auc@stanford.edu},
        Justin~Cunningham~\IEEEmembership{jcnnghm@stanford.edu},
        and~Weixiong~Zheng~\IEEEmembership{zhengwx@stanford.edu}}

\markboth{CS 221 (Autumn 2014) Project Final Report}%
{}

\maketitle


\begin{abstract}
The project aims to explore machine learning methods to predict a movie's critical success prior
to it's release, i.e. based solely on metadata available about the movie. The code for this project
can be found at \texttt{\url{https://github.com/jcnnghm/cs221-project}}
\end{abstract}


\section{Overview}
The movie industry generates multi-billion dollars in revenue and most movies
cost millions of dollars to create.  However, not all movies are successes.
With such high risk and large amounts of money involved, it would be useful to
be able to predict the success of a movie before it is released. In this
project, we try to predict the critical success of a movie, as shown by user
ratings on IMDb \cite{imdb}, based solely on metadata about a movie available prior to it's
release.

\begin{itemize}
    \item Input: IMDb metadata for a movie related to it's creation and release
    \item Output: Predicted user rating of a movie, on a scale of 0-10
\end{itemize}


\section{Related Work}
Henning-Thurau, Houston and Walsh \cite{marketing} performed an empirical study
to distinguish direct and indirect relationships among different determinants
of movie success and found that star and director power does not guarantee
success. However, cultural familiarity (e.g. sequels to a successful movie),
release dates (e.g. during the summer), budget and awards were a huge predictor
of success. Relatedly, Deniz and Hasbrouck \cite{greenlight} performed
statistical analysis on the top 150 grossing movies of 2010 and found that
genre, MPAA rating, budget, star power, adaptation from another medium, sequels
and remakes are significant predictors
of box office revenue.
\\
\par There are also other student projects that attempted to predict user
ratings. One was done for the Machine Learning class (CS229) \cite{hitorflop},
where they used Naive Bayes and Support Vector Machines to predict IMDb user
ratings and profitability. Their system performed moderately well on their test
data. Another similar project \cite{cooper} used regression (Support Vector
Regression, Boosted Decision Trees, Gradient Boosting Regression and Random
Forest Regression) over a different dataset (The Sagel Index of the top and
worst 1000 films), predicting audience ratings on Rotten Tomatoes. Their system
resulted in error rates of roughly 10\%.

\section{Dataset}
Our dataset is the list of all movies from IMDb that fulfill the following properties
\begin{itemize}
	\item Released in the US
	\item Generated gross earnings in the US
	\item Has at least 1,000 user votes to rate the movie
\end{itemize}

\par After pruning the database of 3 million entries, we are left with 9,888
movies which is a reasonable number for our algorithms to run on. Limiting the
data to movies with a reasonable number of user votes also ensures that the
rating data is not too noisy. We save 20\% of the data for testing, and use the
rest for development.

\scalebox{0.55}{\includegraphics{ratings.eps}}

The user ratings in our dataset have a mean of 6.5 and a median of 6.6 with a standard deviation of 1.05.

\section{Evaluation Metric}
We used the standard deviation of the predicted user rating from the actual values on IMDb, calculated as

$$ \sqrt{\frac{\sum_{i=1}^n(\text{Predicted}_i-\text{Actual}_i)^2}{n}} $$

\subsection{Baseline}
For our baseline, we included the following features:
\begin{itemize}
	\item Complete cast members, which includes actors, directors, producers, etc.
	\item Movie genre
	\item Keywords describing the movie
\end{itemize}

This results in a standard error of 0.06 on the training data, and
1.78 on the test data.

\subsection{Oracle}
For our oracle, we added additional features which are movie data available
\emph{after} a movie is released. These are:
\begin{itemize}
	\item Gross earnings
	\item IMDb user ratings
	\item Number of votes for the ratings
\end{itemize}

This results in a standard error of of 0.03 on the training data, and 0.34 on the test data.
\\
\par We had initially added critic and audience ratings from Rotten Tomatoes, but it produced unexpected results, namely critic scores were negative weights and negative audience ratings had the opposite effect (movies rated \emph{Spilled} had larger weights than \emph{Upright})

\section{System design}
Our system is composed of a feature creator system, a snapshot of the IMDb dataset,
and a variety of learning algorithms that operate on the information exported by
the feature creator.
\par The feature creator provides a framework that allows sub-component
feature extractors to interact with the IMDb dataset, stored in MySQL.  The feature
creator calls each extractor ($E_i$ in the diagram below) in turn for each of the movies we've selected for our
dev and test sets, and merges their output features into a sparse feature vector for
each movie.  An additional combinator step ($C_i$ in the diagram) is applied to generate feature
combinations, e.g. \{movie director $\times$ movie actors\}.
\par The creator is highly configurable so that extractors and combinators can easily be selectively enabled or disabled,
and includes a caching layer to reduce the load on MySQL and speed up processing.
The feature vectors are stored in JSON so that they can be easily
post-processed for use with different algorithms. \\

\begin{center}
    \includegraphics[width=8cm]{charts/system.eps}
\end{center}

Each algorithm is responsible for transforming the data to the format that it requires.
The neural network, for example, converts the sparse vectors from the feature creator
into a dense representation that can be consumed by FANN \cite{fann}, a neural network library.  Each
component is modular, so that it is easy to tune individual algorithms.


\section{Features}
The features we used can be catogrized into 3 different types.

\par The first type of features are the original data we get from IMDB without any modifications, such as cast, genere, etc. These features form the basis of the feature set for our Machine Learning algorithms. However, some of them don't generalize well. For example, as time goes by, new names would keep showing up in casts. Most of the weights in our models for old casts won't help in predicting ratings for new movies.

\par The second type of features are combined features, such as \{movie director $\times$ movie actors\}. With these features, we want to capture the significance of relationships between some individual features. For example, an actor might work really well with a specific director. These combined features also don't generalize well in some cases.

\par The third type of features are computed features such as cast experience and standarized budget. These features involved thoughtful analysis and computation. These computed features generalized pretty well across training and test samples. These features helped a lot in pushing the boundary of prediction accuracy.

\par Below is a detailed description of each class of features we include along with an example of the features we generate.

\subsection{Individual features}
\subsubsection{Budget}
The Budget feature extractor extracts a weighted feature, within the domain of
\$1 to \$300M.  Since the data is crowdsourced, there are multiple specified
budgets for the movie.  We only consider movies with budgets in US dollars, and
we take the largest proposed budget.  We also consider using budgets in two
ways -- using raw dollar values or bucketing the value into \$5M chunks (as
indicator variables).
\begin{align*}
        \{ \ \ & \\
        &``budget": 10000000,\\
        &``budget\_bucket\_2: 1\\
        \} \ \ &
\end{align*}

\subsubsection{Cast}
The cast feature extractor generates indicator features for each
cast and crew member associated with a film, for each job they
performed.  An individual could potentially appear more than once,
if they performed different jobs, for example, if they were both
an actor and director.  The cast roles which we include are:
\begin{itemize}
	\item Actor
	\item Actress
	\item Producer
	\item Writer
	\item Cinematographer
	\item Composer
	\item Costume designer
	\item Director
	\item Editor
	\item Production designer
	\item Miscellaneous crew
\end{itemize}
\begin{align*}
        \{ \ \ & \\
        &``Lasseter,John\_producer": 1,\\
        &``Cummings,Jim\_actor": 1,\\
        &``Mitchell,Nicole\_writer": 1,\\
        \} \ \ &
\end{align*}

\subsubsection{Genre}
Genre features are indicator features that are activated for each
genre associated with a movie in the IMDb dataset.
\begin{align*}
        \{\ \ &\\
        &``genre\_Family": 1,\\
        &``genre\_Comedy": 1,\\
        \}\ \ &
\end{align*}

\subsubsection{Keywords}
Keyword features are indicator features that are activated for
each searchable tag that is associated with a movie in the
IMDb dataset.
\begin{align*}
        \{\ \ &\\
        &``character-name-in-title": 1,\\
        &``sequel": 1,\\
        &``friendship": 1,\\
        \}\ \ &
\end{align*}

\subsubsection{Release Date}
Release dates are also indicator features that are activated for each release
in the US across festivals and premieres. For each release, we generate 3
features, one for the month, year, and both month and year.
\begin{align*}
        \{\ \ &\\
        &``release\ (traverse\ film\ festival)\ (August)": 1, \\
        &``release\ (traverse\ film\ festival)\ (2014)": 1, \\
        &``release\ (traverse\ film\ festival)\ (August\ 2014)": 1, \\
        &``release\ (premiere)\ (February)": 1, \\
        &``release\ (premiere)\ (2014)": 1, \\
        &``release\ (premiere)\ (February\ 2014)": 1, \\
        \}\ \ &
\end{align*}

\subsubsection{Budget $\times$ Cast}
We use the bucketized budget data and join that with the extracted cast
features. The idea is that an actor's star power will be amplified by a big
budget production, or reduced in an indie movie.
\begin{align*}
        \{\ \ &\\
        &``budget\_bucket\_2\_Cummings,Jim\_actor": 1,\\
        &``budget\_bucket\_2\_Klein,Sebastian\_actor": 1\\
        \}\ \ &
\end{align*}

\subsubsection{Actor $\times$ Director}
We create new features for each combination of actor and director in each cast
list (where actor includes both actors and actresses, treated distinctly in our
dataset). The intuition is that some pairs of actors and directors work well
together and create better movies together.
\begin{align*}
        \{\ \ &\\
        &``Nolan,Christopher\_Cummings,Jim\_actor": 1,\\
        &``Nolan,Christopher\_Klein,Sebastian\_actor": 1\\
        \}\ \ &
\end{align*}

\subsubsection{Standardized budget}
In order to account for inflation, we added a feature which calculates the
standard score of the movie budget based on other movies released that same
year.  The standard score of the movie budget for a movie $M$ released in year
$Y$ is given by

\begin{eqnarray*}
    z = \frac{x - \mu}{\sigma}
\end{eqnarray*}
where
\begin{itemize}
    \item $z$ is the standard score of the budget for movie $M$
    \item $\mu$ is the mean of movie budgets released in year $Y$
    \item $\sigma$ is the standard deviation of movie budgets released in year $Y$
\end{itemize}
\begin{align*}
        \{\ \ &\\
        &``standardized\_budget": 0.57408881370714016,\\
        \}\ \ &
\end{align*}

\subsubsection{Cast and Crew Experience}
\paragraph{Experience}

To better generalize for cast and crew we had not trained on, we added features that would describe how much experience the cast and crew had.  For the buckets of all Actors, Actresses, Producers, Directors, the group of of all Actors and Actresses, and the entire cast and crew, we calculated the mean number of movies they had previously worked on or appeared in.  We also calculated the 1st, 10th, 25th, 50th, 75th, 90th, and 99th percentiles for each group.
\begin{center}
    \scalebox{0.5}{\includegraphics{charts/appearances.eps}}
\end{center}

The total number of movies a person had worked on was capped at 20, and each role-value pair was treated as an indicator feature, with values rounded to the nearest integer.
\begin{align*}
        \{\ \ &\\
        &``experience\_Actors\_mean\_14": 1,\\
        &``experience\_ActorsAndActresses\_mean\_16": 1,\\
        &``experience\_Actors\_10\_pctl\_3": 1,\\
        &``experience\_Actors\_90\_pctl\_16": 1,\\
        \}\ \ &
\end{align*}

\paragraph{Quality}

Similar to cast and crew experience, we added features that would describe the quality of the cast and crew based on the rating of movies that they had worked on or appeared in previously.  The mean and percentiles approach as described above was also used for cast and crew quality.
\begin{center}
    \scalebox{0.45}{\includegraphics{charts/crew_quality.eps}}
\end{center}

After the mean or percentile value was calculated, it was rounded to the nearest integer, and along with the role, added as an indicator feature.
\begin{align*}
        \{\ \ &\\
        &``rating\_Actors\_mean\_6": 1,\\
        &``rating\_ActorsAndActresses\_mean\_7": 1,\\
        &``rating\_Actors\_10\_pctl\_4": 1,\\
        &``rating\_Actors\_90\_pctl\_8": 1,\\
        \}\ \ &
\end{align*}

\subsubsection{K-means}
The k-means cluster for each movie is added to each movie's feature vector.
Adding just 10 clusters provided a significant performance improvement after
1,000 iterations of SGD.  The k-means algorithm was initialized with 128 different
centroid seeds using the k-means++ method, the best result is retained.  Total
runtime was less than 4 minutes.  All of our algorithms included k-means clusters as features.

\begin{center}
\begin{tabular}{|r| r|} % columns
\hline
Num. clusters & Test Error  \\ [0.5ex] % inserts table
\hline
0 & 1.75 \\
10 & 0.97 \\
100 & 1.02 \\
1000 & 1.32 \\
\hline %inserts single line
\end{tabular}
\end{center}

Clusters added were treated as indicator features.
\begin{align*}
        \{\ \ &\\
        &``cluster\_4": 1,\\
        \}\ \ &
\end{align*}

\subsection{Analysis of feature sets}
In total, there are about 192,000 unique features without combined features for
our data set.  With combined features, this number balloons to over 800,000.
\begin{center}
    \scalebox{0.55}{\includegraphics{charts/features.eps}}
\end{center}

When represented sparsely, the feature size without combinations is about 53MB, and 82MB with combinations.  The same data represented
densely consumes approximately 3.5GB and 14GB respectively, indicating that many of the features are not associated with many movies.  We chose to prune features that do not represent at least 3 movies, when
those are filtered out, we're left with approximately 120,000 features, with the average feature
representing 20.2 movies. 97.9\% of our features represent fewer than 100 movies.  The data
is skewed heavily to the left, which has the tendency to cause overfitting, which we address in
our algorithms. \\
\\
    \scalebox{0.45}{\includegraphics{charts/movies_per_feature.eps}}
\\
\par An initial investigation into the importance of each feature was performed by
running our baseline algorithm with each feature set individually.\\

\begin{center}
\begin{tabular}{|l|r r|} % columns
\hline
Feature & Dev error & Test error \\ [0.5ex] % inserts table
\hline % inserts single horizontal line
Budget (raw) & 5.89 & 5.87 \\ % inserting body of the table
Budget (bucketized) & 4.62 & 4.68 \\
Cast & 0.65 & 2.56 \\
Genre & 2.26 & 2.29 \\
Keywords & 1.65 & 3.35 \\
Release date & 1.69 & 2.26 \\
\hline
Budget $\times$ Cast & 4.54 & 5.65 \\
Actor $\times$ Director & 2.67 & 5.96 \\
\hline
Standardized budget & 5.02 & 5.03 \\
Cast experience & $\infty$ & $\infty$ \\
\hline
\end{tabular}
\end{center}
\smallskip

\par Here we note that some feature sets are overfitted to our dev dataset such as
cast, keywords, release date, budget $\times$ cast and actor $\times$ director.
These are also our largest feature sets.\\
\\
\begin{tabular}{|l| r r r|} % columns
\hline
Feature             & \shortstack[c]{Num.\\vars} & \shortstack[c]{Num.\\movies} & \shortstack[c]{Feature / \\ movie}  \\ [0.5ex] % inserts table
\hline
Budget (raw)        & 1 & 5,278 (53\%) & 1 \\ % inserting body of the table
Budget (bucketized) & 52 & 5,278 (53\%) & 1 \\
Cast                & 169,973 & 9,883 (99\%) & 72 \\
Genre               & 25 & 9,885 (99\%) & 2 \\
Keywords            & 11,701 & 9,774 (98\%) & 75 \\
Release date        & 10,500 & 9,884 (99\%) & 7 \\
\hline
Budget $\times$ Cast & 394,661 & 5,278 (53\%) & 92 \\
Actor $\times$ Director & 282,672 & 8,630 (87\%) & 35 \\
\hline
Standardized budget & 1 & 5,278 (53\%) & 1 \\
Cast experience & 1,230 & 9,886 (99\%) & 94 \\
\hline %inserts single line
\end{tabular}

\subsection{Feature pruning}
With combined features, our feature space size is around 800,000. It takes hours to train a Linear Regression Model using scikit-learn \cite{scikit} on such a large sample space. We did some experiments on feature removal to improve algorithm performance.
\par In our experiments, we removed all the features that appear less or equal to $n$ times in the training data before training a model. We measured the number of features left after removal and the standard error on test set.\\
\begin{center}
\begin{tabular}{|l| r r|} % columns
\hline
$n$ & Num. features left & Standard Error  \\ [0.5ex] % inserts table
\hline
0 & 723,000 & N/A \\
1 & 188,000 & 0.79 \\
2 & 93,000  & 0.83 \\
9 & 28,000  & 1.12 \\
\hline %inserts single line
\end{tabular}
\end{center}
\smallskip

\textit{delete or update this paragraph?}
\par Removing the features that
appear only once in the training set gives huge improvements on performance
while maintaining low standard errors on fitting the test set. This makes sense
because if a
feature appears only once in the training set, it's highly likely that the
feature won't appear in the test set, which means that the weight for this feature
won't affect the result of fitting the test set. Also, according to the experiment,
increasing the threshold $n$ doesn't give too much performance gain but worsens
the test error.

\subsection{Principal Component Analysis}
We ran Principal Component Analysis, PCA, on our features to further reduce
our feature size for the neural network. With PCA, we
are able to reduce our feature set to 5,000.  The first feature
represents about 1.5\% of the variance, with the first 5,000 features
capturing 91\% of the variance in the data.
\\
\scalebox{0.575}{\includegraphics{charts/pca.eps}}
\\
\scalebox{0.55}{\includegraphics{charts/pca_variance.eps}}

\section{Algorithms}

\subsection{Linear Regression}
We used the implementation of Linear Regression in scikit-learn with default
parameters. The algorithm took 6 seconds to run when the data was represented
sparsely, and gave a standard error of 0.81.

\subsection{Stochastic Gradient Descent}

We obtained the best results running SGD with a regularization constant of 0.01 over 100,000 iterations.  Total runtime was approximately 21 minutes, and peak memory consumption was approximately 20GB.

We were able to obtain a standard error of 0.7197, over 1 point better than our baseline,
and only 0.38 from our oracle.

\subsubsection{Regularization}

We experimented with different regularization constants to prevent overfitting. Running SGD with a regulartization contant smaller than 0.01 leads to severe overfitting. When regularization constant is at around 0.05, test error and training error seem to converge. When regularization constant is at 0.01, although training error and test error haven't totally converged, we get the smallest training error. So we decided to use 0.01 as regularization constant for SGD.

\scalebox{0.55}{\includegraphics{charts/sgd_iterations.eps}}
\scalebox{0.55}{\includegraphics{charts/sgd_regularization.eps}}

\subsection{Logistic Regression}

We ran logistic regression using scikit-learn with default parameters.  The algorithm exhibited overfitting, with a training error of 0.0 and a relatively high test error of 0.9157.  We chose to focus on SGD instead of logistic regression because our data is prone to overfitting, and we were able to obtain better preliminary results and had greater control over the hyperparameters in the SGD implementation.  The total runtime was approximately 85 seconds.

\subsection{SVM}
\textit{This needs to be updated, but I'm not sure how it was run originally.  It might be better to just drop it.}
We used the implementation of SVM in scikit-learn with radial basis function (rbf) kernel and linear kernel. We used the feature removal threshold of 1. SVM with rbf kernel took 3.3 hours to run and gave standard error of 1.09. SVM with linear kernel took 3 minutes to run and gave standard error of 1.02.

\subsection{Neural Network}
\textit{This still needs to be updated.  I am waiting for the network to finish running.} \\ \\
The neural network is implemented using the Fast Artificial Neural Network (FANN)
API.  With two hidden layers, with 20 neurons in the first and 10 in the second,
we have achieved a test error rate of 0.012, when the rating is represented as a
number between 0 and 1.0, corresponding to an average error of approximately 1.1 out of 10.  \\
\\
\scalebox{0.55}{\includegraphics{charts/neural_network.eps}}

\section{Results}
\smallskip
\begin{center}
\begin{tabular}{|l | l|} % columns
\hline
Model               & Standard Error  \\ [0.5ex] % inserts table
\hline
Baseline            & 1.78 \\
\hline
Linear regression  & 0.81 \\
Logistic regressor  & 0.92 \\
SGD regressor       & 0.82 \\
Neural network      & 0.83 \\
SGD with $ \lambda = 0.01 $ \& 100k iterations & 0.72 \\
% SVM (rbf kernel)    & 1.09 \\
% SVM (linear kernel) & 1.02 \\
\hline
Oracle              & 0.34 \\
\hline %inserts single line
\end{tabular}
\end{center}
\smallskip


\section{Analysis}

\subsection{Algorithms}
Linear algorithms are quick to train (60 seconds compared with over 10 hours
for a neural network without PCA) and are not as susceptible to overfitting.
They are also not as resource intensive to compute.

\par Additionally, some benefits of the specific implementations of linear algorithms we used are:
\begin{itemize}
    \item Easier to avoid overfitting with regularization hyperparameters
    \item Sparse matrix support, data sizes are 1GB
\end{itemize}

While we found that with the FANN neural networks:
\begin{itemize}
    \item Limited tunable hyperparameters
    \item Difficult to avoid overfitting -- early stopping heuristic does a decent job however
    \item Dense matrices required
\end{itemize}

\par Neural Networks are able to learn more complex models, but this additional
power can lead to overfitting and massive resource requirements.

\par Reducing the feature set and stopping early helps with this, but in this
case, these compromises result in performance comparable to the linear
algorithms.

\subsection{Features}

We used three different types of features to train the models, base features, combined features, and computed features. Adding combined features expanded the feature space size by more than 4 times, causing a few models to overfit. To handle overfitting and improve performance, we added a preprocess step to remove less common features. By removing the features that appear less than 3 times among all movies, we effectively reduced the feature space size by a factor of 5. However, one downside of the base features and combined features is they don't generalize too well. A lot of these features only appear a few times through out the whole dataset.

\par The computed features generalize very well. For example, cast experience is a feature that can be computed for every movie. Adding these computed features really helped to push the boundary of test errors. By adding the computed features, we successfully reduced test errors by more than 4\%.


\section{Acknowledgements}

% references section

% can use a bibliography generated by BibTeX as a .bbl file
% BibTeX documentation can be easily obtained at:
% http://www.ctan.org/tex-archive/biblio/bibtex/contrib/doc/
% The IEEEtran BibTeX style support page is at:
% http://www.michaelshell.org/tex/ieeetran/bibtex/
%\bibliographystyle{IEEEtran}
% argument is your BibTeX string definitions and bibliography database(s)
%\bibliography{IEEEabrv,../bib/paper}
%
% <OR> manually copy in the resultant .bbl file
% set second argument of \begin to the number of references
% (used to reserve space for the reference number labels box)
\begin{thebibliography}{1}
\bibitem{imdb}
Internet Movie Database: \texttt{\url{http://imdb.com}}

\bibitem{marketing}
H-T Thorsten, H. Mark, W. Gianfranco, \emph{Determinants of Motion Picture Box
Office and Profitability: An Interrelationship Approach} \hskip 1em plus 0.5em
minus 0.4em\relax Review of Managerial Science, 2006.

\bibitem{greenlight}
B.~Deniz and R.~B.~Hasbrouck, \emph{WHEN TO GREENLIGHT:
Examining the Pre-release Factors that Determine Future Box Office Success of a Movie in the United States}
\hskip 1em plus 0.5em minus 0.4em\relax International Journal of Economics and Management Sciences, 2012.

\bibitem{hitorflop}
D.~Cocuzzo and S.~Wu, \emph{Hit or Flop: Box Office Prediction for Feature
Films}\hskip 1em plus
  0.5em minus 0.4em\relax Stanford CS 229 project, Dec 2013.

\bibitem{cooper}
S.~Mevawala and S.~Phadke, \emph{BoxOffice: Machine Learning Methods for Predicting Audience Film Ratings}\hskip 1em plus
  0.5em minus 0.4em\relax The Cooper Union for the Advancement of Science and Art.

\bibitem{fann}
Fast Artificial Neural Network Library: \texttt{\url{http://leenissen.dk/fann/wp/}}

\bibitem{scikit}
Scikit-learn: \texttt{\url{http://scikit-learn.org/stable/}}
\end{thebibliography}

% that's all folks
\end{document}


